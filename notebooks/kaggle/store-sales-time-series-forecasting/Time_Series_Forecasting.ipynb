{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The task is to forecast store sales using the last year's sales data, which includes information about stores (their type, location, etc.), goods families, promotions, national and local events or holidays, transmissions and oil prices. \n",
    "    \n",
    "https://www.kaggle.com/competitions/store-sales-time-series-forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.environ['PROJECT_ROOT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Su5oVLIL-GK5"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from pathlib import Path\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path('data', 'kaggle', 'store-sales-time-series-forecasting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "1jUWQVXZ-HOs",
    "outputId": "28317993-f1b2-44c1-d468-1fd00fb58f0f"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(DATA_ROOT / 'train.csv', index_col = 'id')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_data = pd.read_csv(DATA_ROOT / 'stores.csv')\n",
    "stores_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Tiavh7PK-HYv",
    "outputId": "4d198de7-7161-4413-eefa-45b6201bd547"
   },
   "outputs": [],
   "source": [
    "oil_data = pd.read_csv(DATA_ROOT / 'oil.csv')\n",
    "oil_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays_events_data = pd.read_csv(DATA_ROOT / 'holidays_events.csv')\n",
    "holidays_events_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "NURqer_T-HVH",
    "outputId": "855d832d-b8f2-4e74-989b-3759665e01b0"
   },
   "outputs": [],
   "source": [
    "transactions_data = pd.read_csv(DATA_ROOT / 'transactions.csv')\n",
    "transactions_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_data = stores_data.rename(columns={'city': 'store_city', 'state': 'store_state', \n",
    "                                          'type': 'store_type', 'cluster': 'store_cluster'})\n",
    "train_data = train_data.merge(stores_data, on='store_nbr', how='left')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_data = train_data.merge(oil_data, on='date', how='left')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "holidays_events_data = holidays_events_data.rename(columns={'locale': 'holiday_locale', 'type': 'holiday_type', \n",
    "                                                            'locale_name': 'holiday_locale_name',\n",
    "                                                            'description': 'holiday_description',\n",
    "                                                            'transferred': 'holiday_transferred'})\n",
    "train_data = train_data.merge(holidays_events_data, on='date', how='left')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.merge(transactions_data, on=('date', 'store_nbr'), how='left')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>I chose merging on both columns 'date' and 'store_nbr', because 'train_data' and 'transactions_data' had these two common columns.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> We can see that there are only 6 numerical features, other 10 features are non-numerical.<br>Since there are two columns in the table with similar names, it is necessary to rename them to avoid the confusion while analyzing data.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['family'] = train_data['family'].str.lower()     # for easier reading\n",
    "train_data = train_data.rename(columns={'type_x': 'store_type', 'type_y': 'event_type'})\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = train_data.select_dtypes(include='number').columns.tolist()\n",
    "numerical_features.remove('store_nbr')\n",
    "numerical_features.remove('store_cluster')\n",
    "numerical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = train_data.select_dtypes(exclude='number').columns.tolist()\n",
    "categorical_features.append('store_nbr')\n",
    "categorical_features.append('store_cluster')\n",
    "categorical_features.remove('date')\n",
    "categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6yDCEDYYGveo",
    "outputId": "839affcc-2928-4892-c798-459ff56b59f0"
   },
   "outputs": [],
   "source": [
    "train_data['family'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>As we can see, each family has a big number of observations, that is why it may have sense to build separate models for different categories</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.isna().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>As we can see, only 16.5% observations have information about holiday or event provided that day. Actually, a number of NaN values in 'event_type', 'locale', 'locale_name', 'description', 'transferred' is pretty high, but if we are talking about holidays it seems to be a normal number. I mean, we don't need to remove this features right now. It is necessary to look on how these features influence on the target and make a final decision after that. Talking about filling this NaN values, maybe it is a good idea to make one binary variable, which will tell us if there is a holiday in a particular day or not.\n",
    "<p>'dcoilwtico' coefficient is easier to describe. Two days a week (at the weekend) there is no information about oil price (the prove is below), that is why the coeeficient is near 2/7 (actually, it is little bigger, because the value of 'dcoilwtico' feature can also be NaN during some holidays.\n",
    "<p>Talking about 'transactions' feature, I think that this number of NaN values can be connected with some national or local holidays, when some of the stores could be closed. But it is also necessary to check this guess.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_data_copy = oil_data.copy()\n",
    "oil_data_copy['date'] = pd.to_datetime(oil_data_copy['date'])\n",
    "oil_data_copy['day'] = oil_data_copy['date'].dt.day_name()\n",
    "oil_data_copy['day'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>That is the prove of the fact that there is no oil price at the weekend.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data['dcoilwtico'] = train_data['dcoilwtico'].fillna(method='ffill')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.isna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[train_data['dcoilwtico'].isna()]['date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[train_data['date'] == '2013-01-02'].groupby('date')['dcoilwtico'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['dcoilwtico'] = train_data['dcoilwtico'].fillna(93.14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop_duplicates(subset=['date', 'store_nbr', 'family'], ignore_index=True)\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[train_data['transactions'].isna()].groupby(['family'])['date'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['transactions'] = train_data['transactions'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.isna().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.describe(percentiles=[.25, 0.375, .5, .75, .875, .95]).drop(['count'])\n",
    "\n",
    "# I dropped \"count\" row to get rid of exponentioal number presentation.\n",
    "# And added .375 and .875 percentile to get more detailed information about the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>As we can see from these characteristics, more than 25% of observations have 0 in 'sales' column. Maybe this can be influenced by some categoies, which mostly have zero number of sales. It is an interesting point to explore. <br>What is also interesting about 'sales' column is that 87.5% of observations have less than 640 number of sales, but <b>max</b> value is much bigger. Maybe it is connected with some very popular categories, maybe it is a consequence of some event (I can only guess now, but it can be connected with the earthquake in Ecuador, which happened in Ecuador in 2016. If it is true, such observations could be classified as outliers, because they won't help us with building a predictive model).</i>\n",
    ">A magnitude 7.8 earthquake struck Ecuador on April 16, 2016. People rallied in relief efforts donating water and other first need products which greatly affected supermarket sales for several weeks after the earthquake.\n",
    "(screenshot from the task description in Kaggle)\n",
    "<p><i>The same situation with the 'transactions' feature. There is a much bigger <b>max</b> number of transactions in comparison with 87.5% of 'transactions' values. The reason can be the same as I mentioned before, but it is necessary to check it out.</i>\n",
    "<p><i>One more interesting fact is that the number of promotions is very small in comparison with the whole number of observations (less than 25%). But there is no sense to delete this feature right now, because it seems, that promotions have a good impact on the target.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 40))\n",
    "for i, current_num_feature in enumerate(numerical_features):\n",
    "    plt.subplot(len(numerical_features), 1, i + 1)\n",
    "    plt.xlabel(current_num_feature)\n",
    "    plt.ylabel('count')\n",
    "    x = train_data.groupby('date')[current_num_feature].mean()\n",
    "    plt.hist(x=x, bins=80)\n",
    "    plt.title(current_num_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>I'll describe these 4 plots one by one:<br> 1) 'Sales' plot. Distribution of means is not standard, and it is hard to say something about it, having no connection with datetime. But it interesting to explore values, which are out of the general distribution.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_and_mean_sales = train_data.groupby('date')['sales'].mean()\n",
    "date_and_mean_sales_min = date_and_mean_sales[date_and_mean_sales < date_and_mean_sales.quantile(0.01)]\n",
    "date_and_mean_sales_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>It means, that all the values in the left side of 'sales' distribution refer to New Year. That's why such data are <b>OUTLIERS</b></i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_and_mean_sales_max = date_and_mean_sales[date_and_mean_sales > date_and_mean_sales.quantile(0.993)]\n",
    "date_and_mean_sales_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Most of these dates refer to the first days of the month - days, when people get their wages. This fact explains such high 'sales' values. It can be useful in the future: <b>FIRST DAYS OF MONTHS CAUSE BIG 'SALES' NUMBER</b>. What's more, we have an additional information in the task description:</i>\n",
    "> A magnitude 7.8 earthquake struck Ecuador on April 16, 2016. People rallied in relief efforts donating water and other first need products which greatly affected supermarket sales for several weeks after the earthquake.\n",
    "<p><i>It means, that high 'sales' values on '2016-04-17', '2016-04-18', '2016-05-01' can be explained in this way. According to that, we can clasify observations in these 3 days as <b>outliers.</b><br> One interesting day is left: '2016-12-23'. It is not the wages-day and it can't be connected to the eartquake. Let's see, what can it be.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays_events_data[holidays_events_data['date'].isin(('2016-04-03', '2016-10-02', '2016-12-04', '2016-12-18', '2016-12-23',\n",
    "                                                        '2017-01-02', '2017-04-01', '2017-05-01', '2017-06-04', '2017-07-02'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>I'm pretty sure that holidays these days are nothing more than coincidence. It is necessary to check it out, but I have no idea for that right now. (Actually, we will see later that the existance of the holiday doesn't correlate with the 'sales' at all.)</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onpromotion_data = train_data[train_data['date'].isin(('2016-04-03', '2016-10-02', '2016-12-04',\n",
    "                                                       '2016-12-18', '2016-12-23', '2017-01-02',\n",
    "                                                       '2017-04-01', '2017-05-01', '2017-06-04',\n",
    "                                                       '2017-07-02'))].groupby('date')['onpromotion'].mean()\n",
    "onpromotion_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>That is what we need. A mean promotions value on the '2016-12-23' is very high (we can look on the 'onpromotion' plot and check it out). This fact could cause such a big 'sales' value. (in this case, we can't clasify observations that day as <b>OUTLIERS</b>, because they can help us while buiding a forecasting model)</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_price_data = train_data[train_data['date'].isin(('2016-04-02', '2016-04-03', '2016-10-01', '2016-10-02',\n",
    "                                                  '2016-12-03', '2016-12-04', '2016-12-17', '2016-12-18',\n",
    "                                                  '2016-12-22', '2016-12-23', '2017-01-01', '2017-01-02',\n",
    "                                                  '2017-03-31', '2017-04-01', '2017-04-30', '2017-05-01',\n",
    "                                                  '2017-06-03', '2017-06-04', '2017-07-01', '2017-07-02'))].groupby('date')['dcoilwtico'].mean()\n",
    "oil_price_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>As far as Ecuador is an oil-dependent country, it is necessary to check, if there were any shocks in oil prices these days. But here we can see that there were no shocks, that is why oil price didn't have impact on 'sales' values these days.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>2) 'Onpromotion' plot. I don't think that here we can detect some interesting facts. It is necessary to look on the time-dependent plot to learn more about this feature.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_and_mean_promo = train_data.groupby('date')['onpromotion'].mean()\n",
    "date_and_mean_promo_max = date_and_mean_promo[date_and_mean_promo > date_and_mean_promo.quantile(0.99)]\n",
    "date_and_mean_promo_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays_events_data[holidays_events_data['date'].isin(date_and_mean_promo_max.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>All big promotions date back to the last year of the observaions time. And, as we can see, there were no holidays and events those days, that is why it seems that such <b>HIGH 'ONPROMOTION' VALUES HAVE NO CONNECTIONS WITH OTHER FEATURES</b>.\n",
    "<p>3) Oil price is the independent feature in this task. Rather other features depend on oil price than vice versa.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_and_mean_transactions = train_data.groupby('date')['transactions'].mean()\n",
    "date_and_mean_transactions_min = date_and_mean_transactions[date_and_mean_transactions < date_and_mean_transactions.quantile(0.01)]\n",
    "date_and_mean_transactions_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_and_mean_transactions_max = date_and_mean_transactions[date_and_mean_transactions > date_and_mean_transactions.quantile(0.99)]\n",
    "date_and_mean_transactions_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[train_data['date'].isin(date_and_mean_transactions_max.index)].groupby('date')['sales'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>4) 'Transactions' plot. Everything seems logical here. As far as 'sales' number is very low on the 1st January every year, 'transactions' value is also very low these days (in comparison with other values of this feature). As we decided earlier, such observations are outliers.\n",
    "<br>If we look at the highest values of this feature, we see, that all these values date back to the days before Christmas Day. It is logical, because Ecuador is a catolic country, and Christmas Day is one of the most important holidays in the country, which is celebrated on the 25th December. I looked on the 'sales' values these days, and most of them are located on the right edge of the graph, that is why we can clasify observations these days as <b>OUTLIERS</b>.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_nan_data = train_data.loc[train_data.isnull()['transactions']]\n",
    "transactions_nan_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_nan_rows = transactions_nan_data.shape[0]\n",
    "transactions_nan_data['holiday_type'].value_counts().sum() / number_of_nan_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>My guess is wrong. Existence of a holiday does not affect the 'transactions' value. Probably, the information about what is the reason of NaN 'transactions' isn't available for us.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time_axis(ax):\n",
    "    ax.xaxis.set_major_formatter(mdates.YearLocator())\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    ax.xaxis.set_tick_params(which='major', rotation=90)\n",
    "\n",
    "    ax.xaxis.set_minor_formatter(mdates.MonthLocator())\n",
    "    ax.xaxis.set_minor_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    ax.xaxis.set_tick_params(which='minor', rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['date'] = pd.to_datetime(train_data['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 40))\n",
    "for i, current_num_feature in enumerate(numerical_features):\n",
    "    plt.subplot(len(numerical_features), 1, i + 1)\n",
    "    #format_time_axis(ax)\n",
    "    x = train_data.groupby(pd.Grouper(key='date', freq='W'))[current_num_feature].median()\n",
    "    trend = x.rolling(window=7, center=True, min_periods=5).mean()\n",
    "    plt.plot(x)\n",
    "    trend.plot(x=x, linewidth=3)\n",
    "    plt.axvline(pd.to_datetime('2016-04-16'), color='r', linestyle='--', lw=0.5)\n",
    "    plt.xlabel('date')\n",
    "    plt.ylabel('median ' + current_num_feature)\n",
    "    plt.title(current_num_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>These trend plots prove guesses about extreme values of 'sales' and 'transactions' numerical features\n",
    "    <br>'Onpromotion' median is mostly zero, so the graph is uninformative. Let's plot this feature in a separate cell being aggregated with mean.\n",
    "</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "x = train_data.groupby(pd.Grouper(key='date', freq='W'))['onpromotion'].mean()\n",
    "trend = x.rolling(window=7, center=True, min_periods=5).mean()\n",
    "plt.plot(x)\n",
    "trend.plot(x=x, linewidth=3)\n",
    "plt.axvline(pd.to_datetime('2016-04-16'), color='r', linestyle='--', lw=0.5)\n",
    "plt.xlabel('date')\n",
    "plt.ylabel('mean onpromotion')\n",
    "plt.title('onpromotion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>As we can see on this graph, 'onpromotion' feature has a positive dynamic. There are shocks in promo values and it is necessary to check, what do they mean. (one of them can be explained by the earthquake: people united to help injured persons, a lot of goods had big sales to let people buy as much products as it needed)</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Onpromotion in train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(DATA_ROOT / 'test.csv')\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_day_of_last_year = pd.to_datetime(X['date'].unique()[-1]) - pd.Timedelta(days=365)\n",
    "indexer = X[X['date'] >= str(first_day_of_last_year).split(' ')[0]].index\n",
    "X_train = X[X['date'] >= str(first_day_of_last_year).split(' ')[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Percentage of zeros:')\n",
    "print('Train: ', len(X_train[X_train['onpromotion'] == 0].index) / len(X_train.index))\n",
    "print('Test: ', len(test_data[test_data['onpromotion'] == 0].index) / len(test_data.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "for i, df in enumerate([X_train, test_data]):\n",
    "    plt.subplot(1, 2, i + 1)\n",
    "    plt.xlabel('sales')\n",
    "    plt.ylabel('count')\n",
    "    x = df[df['onpromotion'] != 0]['onpromotion']\n",
    "    plt.hist(x=x, bins=80)\n",
    "    plt.title('onpromotion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.groupby('date')['onpromotion'].mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.groupby('date')['onpromotion'].mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.describe(percentiles=[.25, 0.375, .5, .75, .875, .95]).drop(['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.describe(percentiles=[.25, 0.375, .5, .75, .875, .95]).drop(['count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of unique values:  \\n\")\n",
    "print(train_data[categorical_features].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features_hist = ['family', 'store_city', 'holiday_locale_name', 'store_nbr']\n",
    "cat_features_pie = ['store_state', 'store_type', 'holiday_type', 'holiday_locale', 'holiday_transferred', 'store_cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 15))\n",
    "for i, current_cat_column in enumerate(cat_features_pie):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    n = train_data[current_cat_column].value_counts()\n",
    "    plt.pie(n, labels=n.index, radius=1800, autopct='%1.1f%%')\n",
    "    plt.axis('equal')\n",
    "    plt.title(current_cat_column, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 70))\n",
    "for i, cat_feature in enumerate(cat_features_hist):\n",
    "    plt.subplot(len(cat_features_hist), 1, i + 1)\n",
    "    plt.xlabel(cat_feature)\n",
    "    plt.ylabel('count')\n",
    "    sns.countplot(x=train_data[cat_feature], alpha=0.7)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(cat_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlations between variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data_copy = train_data.copy().set_index('date')\n",
    "train_data_copy = pd.get_dummies(train_data_copy)\n",
    "train_data_copy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_coefficients = (train_data_copy.corr().where(np.triu(np.ones(train_data_copy.corr().shape), k=1).astype(bool))\n",
    "                  .stack()\n",
    "                  .sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_coefficients[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_coefficients[:-10:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>The highest correlation coefficients refer to pairs like 'state' - 'city' or 'event_type' - 'description' or 'locale_name' - 'description', which don't give us any new information. As we will see later, all information about holidays is excess, because the correlation between a binary feature indicating the existence of a holiday on a particular day and all other features is very close to zero.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_data_copy = train_data.copy().drop(['holiday_type', 'holiday_locale', 'holiday_locale_name', \n",
    "                                          'holiday_description', 'store_state'], axis=1)\n",
    "train_data_copy['holiday_transferred'] = np.where((train_data_copy['holiday_transferred'] == True), 1, train_data_copy['holiday_transferred'])\n",
    "train_data_copy['holiday_transferred'] = np.where((train_data_copy['holiday_transferred'] == False), 1, train_data_copy['holiday_transferred'])\n",
    "train_data_copy['holiday_transferred'] = train_data_copy['holiday_transferred'].fillna(0)\n",
    "train_data_copy['holiday_transferred'].value_counts()\n",
    "\n",
    "is_holiday = pd.get_dummies(train_data_copy['holiday_transferred'])\n",
    "train_data_copy = pd.concat((is_holiday, train_data_copy), axis=1)\n",
    "train_data_copy = train_data_copy.rename(columns={1: \"is_holiday\"})\n",
    "train_data_copy = train_data_copy.drop([0, 'holiday_transferred'], axis=1)\n",
    "\n",
    "train_data_copy['transactions'] = train_data_copy['transactions'].fillna(0)\n",
    "\n",
    "train_data_copy = pd.get_dummies(train_data_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = train_data_copy.corr()\n",
    "plt.figure(figsize=(30, 25))\n",
    "\n",
    "sns.heatmap(corr[(corr >= 0.3) | (corr <= -0.3)], \n",
    "            cmap='viridis', vmax=1.0, vmin=-1.0, linewidths=0.1,\n",
    "            annot=True, annot_kws={\"size\": 8}, square=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>That is the prove: there is no correlation between 'is_holiday' feature and other features. It means that <b>MAYBE THERE IS NO REASON TO SAVE ALL THE FEATURES, WHICH DESCRIBE HOLIDAYS AND EVENTS</b> - their influence on target and other features is very close to 0.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Some more interesting findings from this graph:\n",
    "    <br>- <b>target ('sales') correlates well with 'onptomotions' feature</b> (it is obvious) <b> with two goods categories: 'beverages' and 'grocery'</b> can be explained in this way: beverages and especially gracery goods are first consumption goods, that is why number of 'sales' is always pretty high.\n",
    "    <br>- some store types, such as 'store_type_A' and 'store_type_C' correlate well with 'transactions' feature. It means that if observation refered to 'store_type_A', 'transactions' value would be bigger than if observation refered to any other 'store_type'. There is an opposite situation, when we are talking about 'store_type_C'.\n",
    "    <br>- some cities correlates well with store types (for example, 'city_Quito' with 'store_type_A'). It means that in Quoto city there are a lot of stores with type 'A' in comparison with other store types. And this fact cause that, for example, 'city_Quoto' correlates well with 'transactions' feature.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_friday_data = train_data[train_data['holiday_description'] == 'Black Friday'].groupby('date')['sales'].mean()\n",
    "black_friday_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependence on the target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>In this part I'd like to explore the connection with the target of the following features: 'family', 'onpromotion' and 'dcoilwtico', because 'onpromotion' feature correlates well with the target, the same as some categories from 'family' feature. It is interesting that oil price doesn't correlate well with the 'sales' feature, because Ecuador economy is oil-dependent. But maybe there is more complex dependency between these two features.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "transactions_and_sales_data = train_data.groupby('date')[['transactions', 'sales']].mean()\n",
    "plt.xlabel('transactions value')\n",
    "plt.ylabel('mean sales')\n",
    "plt.scatter(x='transactions', y='sales', data=transactions_and_sales_data, alpha=0.3)\n",
    "plt.title('transactions and mean sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_and_sales_data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "transactions_and_sales_data = train_data.groupby('date')[['transactions', 'sales']].median()\n",
    "plt.xlabel('transactions value')\n",
    "plt.ylabel('median sales')\n",
    "plt.scatter(x='transactions', y='sales', data=transactions_and_sales_data, alpha=0.3)\n",
    "plt.title('transactions and median sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_and_sales_data.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Here we can see that <b>CORRELATION BETWEEN TRANSACTIONS AND SALES IS PRETTY BIG</b> (if we're talking about mean values), that is why it is necessary to use this feature in our forecasting model.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "oil_and_sales_data = train_data.groupby('date')[['dcoilwtico', 'sales']].mean()\n",
    "plt.xlabel('oil value')\n",
    "plt.ylabel('mean sales')\n",
    "plt.scatter(x='dcoilwtico', y='sales', data=oil_and_sales_data, alpha=0.3)\n",
    "plt.title('oil price and mean sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_and_sales_data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "oil_and_sales_data = train_data.groupby('date')[['dcoilwtico', 'sales']].median()\n",
    "plt.xlabel('oil value')\n",
    "plt.ylabel('median sales')\n",
    "plt.scatter(x='dcoilwtico', y='sales', data=oil_and_sales_data, alpha=0.3)\n",
    "plt.title('oil price and median sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_and_sales_data.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Here we can see that <b>CORRELATION BETWEEN OIL PRICE AND SALES IS BIG.</b> The last time the correlation coefficient was calculated incorrectly because the sales values per day were not averaged. Now everything is logical. Moreover, the second value is more fair, because there we calculated correlation coefficient with the help of the median 'sales' value. (more resistant to outliers)</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "family_values = train_data['family'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 130))\n",
    "for i, family in enumerate(family_values):\n",
    "    plt.subplot(17, 2, i + 1)\n",
    "    average_sales = train_data[train_data['family'] == family].groupby('date')['sales'].median()\n",
    "    trend = average_sales.rolling(window=365, center=True, min_periods=183).mean()\n",
    "    plt.ylabel('mean sales')\n",
    "    plt.axvline(pd.to_datetime('2016-04-16'), color='r', linestyle='--', lw=0.5)\n",
    "    ax = average_sales.plot(alpha=0.5)\n",
    "    ax = trend.plot(ax=ax, linewidth=3)\n",
    "    plt.title(family, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "for i, family in enumerate(['books', 'baby care']):\n",
    "    plt.subplot(1, 2, i + 1)\n",
    "    average_sales = train_data[train_data['family'] == family].groupby('date')['sales'].mean()\n",
    "    trend = average_sales.rolling(window=365, center=True, min_periods=4).mean()\n",
    "    plt.ylabel('mean sales')\n",
    "    plt.axvline(pd.to_datetime('2016-04-16'), color='r', linestyle='--', lw=0.5)\n",
    "    ax = average_sales.plot(alpha=0.5)\n",
    "    ax = trend.plot(ax=ax, linewidth=3)\n",
    "    plt.title(family, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>We can say that the number of sales in general has a positive dynamic (for example, 'automotive', 'bread/bakery', 'grocery i', 'personal care', ...).<br>Some of the product types have a negative dynamic (such as lingerie).<br>Some goods have very interesting sales distribution (books, produce, home care, ladieswear, ...), and we can't say right now, what is the reason of that.<br>Also some goods families have seasonal increase in sales('school and office supplies', 'liquor, wine, beer', 'grocery ii', 'frozen foods'). It is necessary to make deseasonalized plots, they will help better understand trends. I'll do it later.\n",
    "    <p>Now let's have a look on some spikes and drops in 'sales', which don't belong to the general trend.\n",
    "</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>BOOKS</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_and_books_mean = train_data[train_data['family'] == 'books'].groupby('date')['sales'].mean()\n",
    "date_and_books_mean_max = date_and_books_mean[date_and_books_mean > 0]\n",
    "date_and_books_mean_max.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>'2016-10-08' is a date which corresponds to the begginning of books sales.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays_events_data[holidays_events_data['date'] == '2016-10-08']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_promotions = train_data[train_data['family'] == 'books'].groupby('date')['onpromotion'].mean()\n",
    "books_promotions[books_promotions.index == '2016-10-08']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_and_books_mean_max.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_and_books_mean = train_data[train_data['family'] == 'books'].groupby('date')['transactions'].mean()\n",
    "transactions_and_books_mean[transactions_and_books_mean > 0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>!!!</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>It seems to be illogical: when we are talking about books, mean 'transactions' value is more that 0 almost every day since observations started (1682 days). But, as fas as we know, books 'sales' became more than 0 only on '2016-10-08' (287 days in general).</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>LAWN AND GARDEN</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_and_garden_mean = train_data[train_data['family'] == 'lawn and garden'].groupby('date')['sales'].mean()\n",
    "date_and_garden_mean_max = date_and_garden_mean[date_and_garden_mean > 50]\n",
    "date_and_garden_mean_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "garden_data = train_data[train_data['date'].isin(('2017-02-13', '2017-02-14', '2017-05-13'))]\n",
    "garden_data = garden_data[garden_data['family'] == 'lawn and garden']\n",
    "garden_data = garden_data.groupby('date')['onpromotion'].mean()\n",
    "garden_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "garden_full_data = sorted(train_data[train_data['family'] == 'lawn and garden'].groupby('date')['onpromotion'].mean().tolist())\n",
    "garden_full_data[:-10:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays_events_data[holidays_events_data['date'].isin(('2017-02-13', '2017-02-14', '2017-05-13'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[train_data['date'].isin(('2017-02-11', '2017-02-12', '2017-02-13', \n",
    "                                                  '2017-02-14', '2017-05-12', '2017-05-13'))].groupby('date')['dcoilwtico'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.xlabel('sales')\n",
    "plt.ylabel('count')\n",
    "x = train_data[train_data['family'] == 'lawn and garden']['sales']\n",
    "plt.hist(x=x, bins=80)\n",
    "plt.title('lawn and garden sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lawn_and_garden_data = train_data[train_data['family'] == 'lawn and garden']\n",
    "len(all_lawn_and_garden_data[all_lawn_and_garden_data['sales'] > 100].index) / len(all_lawn_and_garden_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lawn_and_garden_max_sales = train_data[train_data['date'].isin(('2017-02-13', '2017-02-14', '2017-05-13'))]\n",
    "lawn_and_garden_max_sales = lawn_and_garden_max_sales[lawn_and_garden_max_sales['family'] == 'lawn and garden']['sales']\n",
    "lawn_and_garden_max_sales[lawn_and_garden_max_sales > 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>These observations are <b>OUTLIERS</b></i>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><b>GROCERY I</b></i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gracery_and_mean_sales_data = train_data[train_data['family'] == 'grocery i'].groupby('date')[['date', 'sales']].mean()\n",
    "gracery_and_mean_sales_data[gracery_and_mean_sales_data['sales'] > 8000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>First four observations are outliers (they connect with the earthquake).</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gracery_data = train_data[train_data['date'] == '2017-04-01']\n",
    "gracery_data = gracery_data[gracery_data['family'] == 'grocery i']\n",
    "gracery_data['onpromotion'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays_events_data[holidays_events_data['date'] == '2017-04-01']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Probably such big 'sales' value on '2017-04-01' is a result of two facts: first day of the month -> people get their wages; pretty big promotion 'value' (only 1% of all observations in the dataset have bigger 'onpromotion' value). That is why it is not an outlier.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 70))\n",
    "for i, family in enumerate(family_values):\n",
    "    plt.subplot(len(family_values) // 3, 3, i + 1)\n",
    "    current_value_data = train_data[train_data['family'] == family].groupby('date')['sales'].mean()\n",
    "    sns.distplot(current_value_data, color='g', bins=100, hist_kws={'alpha': 0.4})\n",
    "    plt.title(family)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>These plots show that, roughly speaking, sales distributions of all families divided on two parts: <br>1) Normal or close to normal (such as 'automotive', 'bread/bakery', 'cleaning', 'eggs', 'grocery', 'lingerie', ...). Interesting fact that most of distributions from this category have right asymmetry (asymmetry coefficient is positive). The prove is below.<br>2) Distribution, where the biggest density is concentrated in zero or near zero. Other data is distributed differently (some values such as 'home and kitchen i', 'ladieswear' have something like normal distributions). It means that such goods categories aren't essential for people, that is why a number of sales during the day mostly is equal to 0.\n",
    "    <p><b>Interesting fact: first consumption goods are normally distributed, and seasonal goods that do not have a constant demand are distributed differently.</b>\n",
    "</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for family in family_values:\n",
    "    skew = train_data[train_data['family'] == family].groupby('date')['sales'].mean().skew()\n",
    "    print(f'{family}: {skew:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Let's exclude zeros from second-type families and re-build distributions.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_type_families = ['baby care', 'books', 'celebration', 'home and kitchen i', 'home and kitchen ii',\n",
    "                        'home care', 'ladieswear', 'magazines', 'pet supplies', 'players and electronics', \n",
    "                        'produce', 'school and office supplies']\n",
    "second_type_families_without_zeros = train_data[(train_data['family'].isin(second_type_families)) & \n",
    "                                                (train_data['sales'] > 0)][['date', 'family', 'sales']]\n",
    "second_type_families_without_zeros.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "for i, family in enumerate(second_type_families):\n",
    "    plt.subplot(len(second_type_families) // 3, 3, i + 1)\n",
    "    current_value_data = second_type_families_without_zeros[second_type_families_without_zeros['family'] == family].groupby('date')['sales'].mean()\n",
    "    sns.distplot(current_value_data, color='g', bins=100, hist_kws={'alpha': 0.4})\n",
    "    plt.title(family)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><b>After excluding 'sales' equal to zero from second-type families, we can see that such categories are also distributed normally.</b></i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for family in family_values:\n",
    "    current_family_data = train_data[train_data['family'] == family][['onpromotion', 'sales']]\n",
    "    print('\\033[91m' + family + '\\033[0m')\n",
    "    print(current_family_data.corr())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><b>Correlation between 'onpromotion' feature and the target depence on the 'family' feature.</b> Promotions related to such  categories as 'school and office supplies', 'produce', 'home care', 'home and kitchen', 'beauty' and 'beverages' have a good impact on the target (the bigger the promotion, the more sales).</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 80))\n",
    "for i, family in enumerate(family_values):\n",
    "    plt.subplot(len(family_values) // 3, 3, i + 1)\n",
    "    current_value_data = train_data[train_data['family'] == family].groupby('date')[['onpromotion', 'sales']].mean()\n",
    "    plt.xlabel('promotions value')\n",
    "    plt.ylabel('mean sales')\n",
    "    plt.scatter(x='onpromotion', y='sales', data=current_value_data, alpha=0.3)\n",
    "    plt.title(family)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>There is no other relationship between these features, except for linear.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 130))\n",
    "for i, current_store_nbr in enumerate(train_data['store_nbr'].unique()):\n",
    "    plt.subplot(len(train_data['store_nbr'].unique()) // 3, 3, i + 1)\n",
    "    average_sales = train_data[train_data['store_nbr'] == current_store_nbr].groupby('date')['sales'].median()\n",
    "    trend = average_sales.rolling(window=365, center=True, min_periods=183).mean()\n",
    "    plt.ylabel('mean sales')\n",
    "    plt.axvline(pd.to_datetime('2016-04-16'), color='r', linestyle='--', lw=0.5)\n",
    "    ax = average_sales.plot(alpha=0.5)\n",
    "    ax = trend.plot(ax=ax, linewidth=3)\n",
    "    plt.title(current_store_nbr, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_copy = train_data.copy()\n",
    "train_data_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 140))\n",
    "for i, family in enumerate(family_values):\n",
    "    ax = plt.subplot(17, 2, i + 1)\n",
    "    for city in train_data_copy['city'].unique():\n",
    "        average_sales = train_data_copy[(train_data_copy['family'] == family) & \n",
    "                                        (train_data_copy['city'] == city)].groupby('date')['sales'].mean()\n",
    "        trend = average_sales.rolling(window=365, center=True, min_periods=183).mean()\n",
    "        trend.plot(ax=ax, linewidth=3, label=city)\n",
    "        plt.ylabel('mean sales')\n",
    "    plt.title(family)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "sns.countplot(x=train_data['store_type'], alpha=0.7, data=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 140))\n",
    "for i, family in enumerate(family_values):\n",
    "    ax = plt.subplot(17, 2, i + 1)\n",
    "    for store_type in train_data_copy['store_type'].unique():\n",
    "        average_sales = train_data_copy[(train_data_copy['family'] == family) &\n",
    "                                        (train_data_copy['store_type'] == store_type)].groupby('date')['sales'].mean()\n",
    "        trend = average_sales.rolling(window=365, center=True, min_periods=183).mean()\n",
    "        trend.plot(ax=ax, linewidth=3, label=store_type)\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        plt.ylabel = 'mean sales'\n",
    "        plt.legend(handles, labels, loc='lower center')\n",
    "    plt.title(family)\n",
    "    plt.legend(train_data_copy['store_type'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>As we can see, there are common trends in different categories, but mean sales values are more or less depending on the type of store. Despite the fact that type A stores are not the majority, the average number of sales in it is higher than in the others.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('./data/kaggle/store-sales-time-series-forecasting/prepared_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Time_Series_Forecasting.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
