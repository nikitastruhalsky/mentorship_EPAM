{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c613a950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.environ['PROJECT_ROOT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43743b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import shap\n",
    "import copy\n",
    "import pdpipe as pdp\n",
    "from lightgbm import LGBMRegressor\n",
    "from pathlib import Path\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from statsmodels.tsa.deterministic import DeterministicProcess, CalendarFourier\n",
    "from sklearn.model_selection import cross_validate, TimeSeriesSplit\n",
    "from mentorship.features.kaggle.storesales.etl import ETLTransformer\n",
    "from mentorship.features.history import cut_history\n",
    "from mentorship.features.tuning import boruta_features_tuning\n",
    "from mentorship.ml.cv.split import DateTimeSeriesSplit\n",
    "from mentorship.ml.cv.util import format_cv_test_scores\n",
    "from mentorship.ml.models.kaggle.storesales.hyperparams.tuning import tune_hyperparams\n",
    "from mentorship.ml.models.kaggle.storesales.boosting import LGBMPipeline\n",
    "from mentorship.ml.models.kaggle.storesales.linear import LinearPipeline\n",
    "from mentorship.ml.models.reg import PositiveRegressor\n",
    "from mentorship.ml.models.common import RecursiveTSEstimator\n",
    "from mentorship.ml.models.kaggle.storesales.estimators import RecursiveTSEstimatorWithZeroCategories\n",
    "from mentorship.ml.tools.kaggle.storesales.submission import make_submission_file\n",
    "from mentorship.ml.tools.kaggle.storesales.curves import plot_learning_curver\n",
    "from mentorship.ml.tools.timeit import timeit\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20f8419",
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_METRICS = [\n",
    "    'neg_mean_squared_log_error',\n",
    "    'neg_root_mean_squared_error',\n",
    "    'neg_mean_absolute_error',\n",
    "    'r2'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50338ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path('data', 'kaggle', 'store-sales-time-series-forecasting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9749904c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(DATA_ROOT / 'train.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92b7c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STORES = train['store_nbr'].nunique()\n",
    "N_FAMILIES = train['family'].nunique()\n",
    "N_TIME_SERIES = N_STORES * N_FAMILIES\n",
    "\n",
    "DAYS_IN_YEAR = 365\n",
    "N_HORIZONS = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed80fd25",
   "metadata": {},
   "source": [
    "\n",
    "# Best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0d5d5f",
   "metadata": {},
   "source": [
    "#### Features: 'store_nbr' (categorical), 'dcoilwtico', 'onpromotion', 'lag'-features (1, 2, 4, 6, 7, 14)\n",
    "#### Models: LinearRegression, LGBMRegressor (depends on the family)\n",
    "#### Loss function: MSLE\n",
    "#### 'zero'-categories: 'baby care', 'books' -- predict constant zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb636a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the train set\n",
    "\n",
    "lags = [1, 2, 4, 6, 7, 14]\n",
    "\n",
    "X = train.copy()\n",
    "y = X['sales'].copy()\n",
    "\n",
    "train_transformer = ETLTransformer(lags=lags, target_col='sales')\n",
    "X = train_transformer.transform(X)\n",
    "splitter = DateTimeSeriesSplit()\n",
    "X_train, y_train = cut_history(X=X, date_column='date', keep_interval=pd.Timedelta(days=DAYS_IN_YEAR), y=y)\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a01d71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 150))\n",
    "X_school = X[X['family'] == 'school and office supplies']\n",
    "X_school['sales'] = np.expm1(X_school['sales'])\n",
    "for i, store_nbr in enumerate(X['store_nbr'].unique()):\n",
    "    plt.subplot(27, 2, i + 1)\n",
    "    average_sales = X_school[X_school['store_nbr'] == store_nbr].groupby('date')['sales'].median()\n",
    "    trend = average_sales.rolling(window=365, center=True, min_periods=183).mean()\n",
    "    plt.ylabel('mean sales')\n",
    "    ax = average_sales.plot(alpha=0.5)\n",
    "    ax = trend.plot(ax=ax, linewidth=3)\n",
    "    plt.title(store_nbr, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f25ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the test set\n",
    "\n",
    "test_data = pd.read_csv(DATA_ROOT / 'test.csv')\n",
    "test_transformer = ETLTransformer()\n",
    "test_data = test_transformer.transform(test_data)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799d0278",
   "metadata": {},
   "outputs": [],
   "source": [
    "### seasonality\n",
    "\n",
    "seasonal_families = ['lingerie', 'school and office supplies']\n",
    "y_seasonalities = {}\n",
    "seasonal_models = {}\n",
    "X['sales'] = np.expm1(X['sales'])\n",
    "for current_family in X['family'].unique():\n",
    "    X_current_family = X[X['family'] == current_family]\n",
    "    if current_family in seasonal_families:\n",
    "        average_sales_current_family = X_current_family.groupby('date')['sales'].median()\n",
    "        X_current_family_seasonal = X_current_family.drop_duplicates(subset=['date'])\n",
    "        y_current_family = y.loc[X_current_family_seasonal.index]\n",
    "\n",
    "        X_current_family_dates = X_current_family_seasonal['date']\n",
    "        test_current_family_dates = test_data[test_data['family'] == current_family].drop_duplicates(subset=['date'])['date']\n",
    "        train_and_test_dates = pd.concat([X_current_family_dates, test_current_family_dates])\n",
    "        train_and_test_dates = pd.to_datetime(train_and_test_dates).dt.to_period('D')\n",
    "        \n",
    "        fourier = CalendarFourier(freq='A', order=10)  # 10 sin/cos pairs for \"A\"nnual seasonality\n",
    "        dp_train_and_test = DeterministicProcess(index=train_and_test_dates, constant=True, additional_terms=[fourier], \n",
    "                                                 drop=True)\n",
    "        \n",
    "        X_current_family_train_season = dp_train_and_test.in_sample()[:-N_HORIZONS]\n",
    "        X_current_family_test_season = dp_train_and_test.in_sample()[-N_HORIZONS:]\n",
    "        seasonal_models[current_family] = (LinearRegression().fit(X_current_family_train_season, average_sales_current_family), \n",
    "                                           X_current_family_test_season)\n",
    "        y_fit_current_family = pd.DataFrame(data={'season': seasonal_models[current_family][0].predict(X_current_family_train_season), \n",
    "                                                  'date': X_current_family['date'].unique()})\n",
    "        X_current_family = X_current_family.reset_index().merge(y_fit_current_family, how='left', on='date').set_index('index')\n",
    "        y_seasonalities[current_family] = X_current_family['season']\n",
    "    else:\n",
    "        y_seasonalities[current_family] = pd.Series(data=0, index=X_current_family.index)\n",
    "y_season = pd.concat(y_seasonalities.values()).loc[X.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d7cadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_categories = ['baby care', 'books', 'lawn and garden', 'home appliances']\n",
    "fit_params = {'feature_name': ['store_nbr', 'onpromotion', 'dcoilwtico', 'lag_1', 'lag_2', 'lag_4', 'lag_6', 'lag_7', 'lag_14'],\n",
    "              'categorical_feature': ['store_nbr']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85819afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "### trend\n",
    "\n",
    "# linear_trend_families = ['beverages', 'eggs', 'grocery i', 'meats', 'pet supplies', 'produce']\n",
    "# quadr_trend_families = ['grocery ii', 'prepared foods']\n",
    "# y_trends = {}\n",
    "# trend_models = {}\n",
    "# X['sales'] = np.expm1(X['sales'])\n",
    "# for current_family in X['family'].unique():\n",
    "#     X_current_family = X[X['family'] == current_family]\n",
    "#     if current_family in linear_trend_families + quadr_trend_families:\n",
    "#         average_sales_current_family = X_current_family.groupby('date')['sales'].median()\n",
    "#         X_current_family_trend = X_current_family.drop_duplicates(subset=['date'])\n",
    "#         y_current_family = y.loc[X_current_family_trend.index]\n",
    "\n",
    "#         if current_family in linear_trend_families:\n",
    "#             dp_train_and_test = DeterministicProcess(index=list(y_current_family.index) + list(range(N_HORIZONS)), order=1, drop=True)\n",
    "#         else:\n",
    "#             dp_train_and_test = DeterministicProcess(index=list(y_current_family.index) + list(range(N_HORIZONS)), order=2, drop=True)\n",
    "\n",
    "#         X_current_family_train_trend = dp_train_and_test.in_sample()[:-N_HORIZONS]\n",
    "#         X_current_family_test_trend = dp_train_and_test.in_sample()[-N_HORIZONS:]\n",
    "#         trend_models[current_family] = (LinearRegression().fit(X_current_family_train_trend, average_sales_current_family), \n",
    "#                                         X_current_family_test_trend)\n",
    "#         y_fit_current_family = pd.DataFrame(data={'trend': trend_models[current_family][0].predict(X_current_family_train_trend), \n",
    "#                                                   'date': X_current_family['date'].unique()})\n",
    "#         X_current_family = X_current_family.reset_index().merge(y_fit_current_family, how='left', on='date').set_index('index')\n",
    "#         y_trends[current_family] = X_current_family['trend']\n",
    "#     else:\n",
    "#         y_trends[current_family] = pd.Series(data=0, index=X_current_family.index)\n",
    "# y_trend = pd.concat(y_trends.values()).loc[X.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e7ebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "X_school = X[X['family'] == 'school and office supplies']\n",
    "average_sales = X_school.groupby('date')['sales'].median()\n",
    "plt.ylabel('mean sales')\n",
    "ax = average_sales.plot(alpha=0.5)\n",
    "ax = X_school.groupby('date')['season'].median().plot(ax=ax, linewidth=3)\n",
    "plt.title('school and office supplies', color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629db06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.loc[:, 'sales_deseasonalized'] = X['sales'] - y_season\n",
    "y_deseasonalized = X['sales'] - y_season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cade389f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 130))\n",
    "for i, family in enumerate(X['family'].unique()):\n",
    "    plt.subplot(17, 2, i + 1)\n",
    "    average_sales = X[X['family'] == family].groupby('date')['sales_deseasonalized'].median()\n",
    "    trend = average_sales.rolling(window=365, center=True, min_periods=183).mean()\n",
    "    plt.ylabel('mean sales')\n",
    "    ax = average_sales.plot(alpha=0.5)\n",
    "    ax = trend.plot(ax=ax, linewidth=3)\n",
    "    plt.title(family, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b43bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_deseasonalized_families_ind = X[~X['family'].isin(seasonal_families)].index\n",
    "X.loc[not_deseasonalized_families_ind, 'sales_deseasonalized'] = np.log1p(X.loc[not_deseasonalized_families_ind, 'sales_deseasonalized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892d7e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "for current_lag in lags:\n",
    "    X.loc[:, 'lag_{}'.format(current_lag)] = X.groupby(['store_nbr', 'family'])['sales_deseasonalized'].shift(current_lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1cb90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing best cross-validation and final params\n",
    "import json\n",
    "\n",
    "with open(DATA_ROOT / 'best_cv_params.txt') as f:\n",
    "    data = f.read()\n",
    "best_cv_params = json.loads(data)\n",
    "\n",
    "with open(DATA_ROOT / 'best_final_params.txt') as f:\n",
    "    data = f.read()\n",
    "best_final_params = json.loads(data)\n",
    "        \n",
    "with open(DATA_ROOT / 'best_cv_params_detrended.txt') as f:\n",
    "    data = f.read()\n",
    "best_cv_params_detrended = json.loads(data)\n",
    "\n",
    "with open(DATA_ROOT / 'best_final_params_detrended.txt') as f:\n",
    "    data = f.read()\n",
    "best_final_params_detrended = json.loads(data)\n",
    "\n",
    "with open(DATA_ROOT / 'best_cv_params_deseasonalized.txt') as f:\n",
    "    data = f.read()\n",
    "best_cv_params_deseasonalized = json.loads(data)\n",
    "\n",
    "with open(DATA_ROOT / 'best_final_params_deseasonalized.txt') as f:\n",
    "    data = f.read()\n",
    "best_final_params_deseasonalized = json.loads(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023c067e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tscv_inner = TimeSeriesSplit(gap=0, max_train_size=(DAYS_IN_YEAR - 4 * N_HORIZONS) * N_STORES, n_splits=4,\n",
    "                             test_size=N_HORIZONS * N_STORES)\n",
    "train_indices = next(splitter.split(X, y))[0]\n",
    "\n",
    "X_train_first_fold, y_train_first_fold = X.iloc[train_indices], y_deseasonalized.iloc[train_indices]\n",
    "\n",
    "X_train_deseasonalized_families = X_train_first_fold[X_train_first_fold['family'].isin(seasonal_families)]\n",
    "y_train_deseasonalized_families = y_train_first_fold.loc[X_train_deseasonalized_families.index]\n",
    "\n",
    "best_cv_params_deseasonalized = tune_hyperparams(X_train_deseasonalized_families.drop(columns=['sales']), \n",
    "                                                 y_train_deseasonalized_families, tscv_inner=tscv_inner, lags=lags, \n",
    "                                                 level='store_nbr', target_col='sales_deseasonalized', \n",
    "                                                 zero_categories=['baby care', 'books'], predict_negative=True, \n",
    "                                                 use_final_metric=False, scoring='neg_root_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f030f10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_pipelines = {}\n",
    "for current_family in X['family'].unique():\n",
    "    if current_family in linear_categories:\n",
    "        base_pipelines[current_family] = LinearPipeline(cols_to_scale=['dcoilwtico'], cols_to_encode=['store_nbr'], \n",
    "                                                        drop_columns=['onpromotion'], lags=lags, split_key='family', \n",
    "                                                        target_col='sales_deseasonalized', level='store_nbr')\n",
    "    else:\n",
    "        if current_family in seasonal_families:\n",
    "            base_pipelines[current_family] = LGBMPipeline(lags=lags, split_key='family', target_col='sales_deseasonalized', \n",
    "                                                          level='store_nbr', params=best_cv_params_deseasonalized[current_family], \n",
    "                                                          fit_params=fit_params, use_final_metric=False, predict_negative=True)\n",
    "        else:\n",
    "            base_pipelines[current_family] = LGBMPipeline(lags=lags, split_key='family', target_col='sales_deseasonalized', \n",
    "                                                          level='store_nbr', params=best_cv_params[current_family],\n",
    "                                                          fit_params=fit_params, use_final_metric=True, predict_negative=False)\n",
    "\n",
    "modelling_pipeline = RecursiveTSEstimatorWithZeroCategories(split_key='family', target_col='sales_deseasonalized', \n",
    "                                                            trend_pred=y_season, base_pipelines=base_pipelines,\n",
    "                                                            zero_categories=['baby care', 'books'], y_detrended=y_deseasonalized)\n",
    "\n",
    "scores = cross_validate(modelling_pipeline, X.drop(columns=['sales']), y, cv=splitter,\n",
    "                        scoring=CV_METRICS, return_estimator=True, error_score='raise', n_jobs=-1)\n",
    "format_cv_test_scores(scores, metrics_to_plot=['root_mean_squared_log_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8479c28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_for_every_family = {}\n",
    "tscv = TimeSeriesSplit(gap=0, max_train_size=DAYS_IN_YEAR * N_STORES, n_splits=4, test_size=N_HORIZONS * N_STORES)\n",
    "\n",
    "for current_family in X['family'].unique():\n",
    "    X_current_family = X[X['family'] == current_family]\n",
    "    y_current_family = y.loc[X_current_family.index]\n",
    "    \n",
    "    base_pipeline = {current_family:base_pipelines[current_family]}\n",
    "    modelling_pipeline = RecursiveTSEstimatorWithZeroCategories(split_key='family', target_col='sales_deseasonalized', \n",
    "                                                                trend_pred=y_season, base_pipelines=base_pipeline,\n",
    "                                                                zero_categories=['baby care', 'books'], y_detrended=y_deseasonalized)\n",
    "    \n",
    "    scores = cross_validate(modelling_pipeline, X_current_family.drop(columns=['sales']), y_current_family, cv=tscv, \n",
    "                            scoring=CV_METRICS, return_estimator=True, error_score='raise', n_jobs=-1)\n",
    "\n",
    "    scores_for_every_family[current_family] = format_cv_test_scores(scores, save_scores=True, print_scores=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2385871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model after deseasonalizing\n",
    "\n",
    "plot_scores_for_every_feature_value(scores_for_every_family, 'family')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed496eda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train = cut_history(X=X, date_column='date', keep_interval=pd.Timedelta(days=DAYS_IN_YEAR), y=y_deseasonalized)\n",
    "tscv_inner = TimeSeriesSplit(gap=0, max_train_size=(DAYS_IN_YEAR - 4 * N_HORIZONS) * N_STORES, n_splits=4,\n",
    "                             test_size=N_HORIZONS * N_STORES)\n",
    "\n",
    "X_train_deseasonalized_families = X_train[X_train['family'].isin(seasonal_families)]\n",
    "y_train_deseasonalized_families = y_train.loc[X_train_deseasonalized_families.index]\n",
    "\n",
    "best_final_params_deseasonalized = tune_hyperparams(X_train_deseasonalized_families.drop(columns=['sales']), \n",
    "                                                    y_train_deseasonalized_families, tscv_inner=tscv_inner, lags=lags, \n",
    "                                                    level='store_nbr', target_col='sales_deseasonalized', \n",
    "                                                    zero_categories=['baby care', 'books'], predict_negative=True, \n",
    "                                                    use_final_metric=False, scoring='neg_root_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926500fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and testing the final model; saving the predictions\n",
    "\n",
    "X_train, y_train = cut_history(X=X, date_column='date', keep_interval=pd.Timedelta(days=DAYS_IN_YEAR), y=y)\n",
    "\n",
    "base_pipelines = {}\n",
    "for current_family in X['family'].unique():\n",
    "    if current_family in linear_categories:\n",
    "        base_pipelines[current_family] = LinearPipeline(cols_to_scale=['dcoilwtico'], cols_to_encode=['store_nbr'], \n",
    "                                                        drop_columns=['onpromotion', 'sales_deseasonalized'], lags=lags, \n",
    "                                                        split_key='family', target_col='sales_deseasonalized', level='store_nbr')\n",
    "    else:\n",
    "        if current_family in seasonal_families:\n",
    "            base_pipelines[current_family] = LGBMPipeline(lags=lags, split_key='family', target_col='sales_deseasonalized', \n",
    "                                                          level='store_nbr', params=best_final_params_deseasonalized[current_family], \n",
    "                                                          fit_params=fit_params, use_final_metric=False, predict_negative=True)\n",
    "        else:\n",
    "            base_pipelines[current_family] = LGBMPipeline(lags=lags, split_key='family', target_col='sales_deseasonalized', \n",
    "                                                          level='store_nbr', params=best_final_params[current_family], \n",
    "                                                          fit_params=fit_params, use_final_metric=True, predict_negative=False)\n",
    "\n",
    "final_modelling_pipeline = RecursiveTSEstimatorWithZeroCategories(split_key='family', target_col='sales_deseasonalized',\n",
    "                                                                  base_pipelines=base_pipelines, y_detrended=y_deseasonalized,\n",
    "                                                                  zero_categories=['baby care', 'books'], \n",
    "                                                                  detrended_categories=seasonal_families, \n",
    "                                                                  trend_models=seasonal_models)\n",
    "\n",
    "final_modelling_pipeline.fit(X_train.drop(columns=['sales']), y_train)\n",
    "make_submission_file(test_data, final_modelling_pipeline, 'optuna_LGBM_and_linreg.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ed0517",
   "metadata": {},
   "source": [
    "### Score on Kaggle: 0.4185"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e714081",
   "metadata": {},
   "source": [
    "#### Compare scores for every family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0ac801",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_pipelines = {}\n",
    "linear_categories = ['baby care', 'books', 'lawn and garden', 'home appliances']\n",
    "params = {'categorical_feature': 'name: \"store_nbr\"'}\n",
    "zero_categories = ['baby care', 'books']\n",
    "\n",
    "for current_family in X['family'].unique():\n",
    "    if current_family in linear_categories:\n",
    "        base_pipelines[current_family] = LinearPipeline(cols_to_scale=['dcoilwtico'], cols_to_encode=['store_nbr'], \n",
    "                                                        drop_columns=['onpromotion'], lags=lags, split_key='family', \n",
    "                                                        target_col='sales', level=['store_nbr'])\n",
    "    else:\n",
    "        base_pipelines[current_family] = LGBMPipeline(lags=lags, split_key='family', target_col='sales', level=['store_nbr'],\n",
    "                                                      params=best_cv_params[current_family].update(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d64e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_for_every_family = {}\n",
    "tscv = TimeSeriesSplit(gap=0, max_train_size=DAYS_IN_YEAR * N_STORES, n_splits=4, test_size=N_HORIZONS * N_STORES)\n",
    "\n",
    "for current_family in X['family'].unique():\n",
    "    X_current_family = X[X['family'] == current_family]\n",
    "    y_current_family = y.loc[X_current_family.index]\n",
    "    \n",
    "    base_pipeline = {current_family:base_pipelines[current_family]}\n",
    "    modelling_pipeline = RecursiveTSEstimatorWithZeroCategories(base_pipelines=base_pipeline, split_key='family', \n",
    "                                                                target_col='sales', zero_categories=zero_categories)\n",
    "    \n",
    "    scores = cross_validate(modelling_pipeline, X_current_family, y_current_family, cv=tscv, scoring=CV_METRICS, \n",
    "                            return_estimator=True, error_score='raise', n_jobs=-1)\n",
    "\n",
    "    scores_for_every_family[current_family] = format_cv_test_scores(scores, save_scores=True, print_scores=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8768dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores_for_every_feature_value(scores, feature):\n",
    "    rmsle_for_every_feature_value = {}\n",
    "    for key in scores.keys():\n",
    "        rmsle_for_every_feature_value[key] = round(scores[key][0][\"root_mean_squared_log_error\"], 3)\n",
    "\n",
    "    sorted_rmsle_for_every_feature_value = {str(k): v for k, v in sorted(rmsle_for_every_feature_value.items(), \n",
    "                                                                    key=lambda item: item[1], reverse=True)}\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.bar(sorted_rmsle_for_every_feature_value.keys(), sorted_rmsle_for_every_feature_value.values())\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.axhline(y=0.4185, color='red', linestyle='dashed')\n",
    "    plt.title(feature)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c8d958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model before deseasonalizing\n",
    "\n",
    "plot_scores_for_every_feature_value(scores_for_every_family, 'family')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba6ad81",
   "metadata": {},
   "source": [
    "<b><p>1) Model probably makes mistakes in 'grocery ii' category, because of the distribution of its sales (it rises up sharply in last months); in 'school and office supplies' category (there is a period in the last year with extremely high sales, which could affect on the boosting algorithm).</p>\n",
    "    <p><i>Possible solution:</i> for 'grocery ii' category train the model only on the recent 'high values' data (last months); for 'school and office supplies' use some seasonality tools or other model (Prophet, for example) (it can be seen that sales peaks are seasonal, which is logical for such category).</p></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3a9e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "last = next(splitter.split(X, y))\n",
    "for last in splitter.split(X, y):\n",
    "    continue\n",
    "    \n",
    "train_indices = last[0]\n",
    "\n",
    "X_train_last_fold, y_train_last_fold = X.iloc[train_indices], y.iloc[train_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65189102",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 90))\n",
    "for i, current_family in enumerate(X['family'].unique()):\n",
    "    X_current_family = X_train_last_fold[X_train_last_fold['family'] == current_family].drop(columns=['date', 'sales', 'family'])\n",
    "    y_current_family = y_train_last_fold.loc[X_current_family.index]\n",
    "    \n",
    "    plt.subplot(11, 3, i + 1)\n",
    "    lgbm_model = lgb.LGBMRegressor(importance_type='gain')\n",
    "    lgbm_model.fit(X_current_family, y_current_family, categorical_feature=['store_nbr'])\n",
    "    lgbm_feature_imp = pd.DataFrame(sorted(zip(lgbm_model.feature_importances_, X_current_family.columns)), \n",
    "                                    columns=['Value', 'Feature'])\n",
    "    sns.barplot(x='Value', y='Feature', data=lgbm_feature_imp.sort_values(by='Value', ascending=False))\n",
    "    plt.title(f'{current_family} (feature_importances_)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd9edec",
   "metadata": {},
   "source": [
    "<b><p>2) As we can see on these plots, features 'lag_2', 'lag_4', 'lag_6' aren't very useful in most categories (by LGBM feature_importance). Probably it could be useful to try some categories without some of these 3 features.</p>\n",
    "   <p>P.S. We don't pay attention to linear categories here.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afc8bc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, current_family in enumerate([family for family in X['family'].unique() if family not in linear_categories]):\n",
    "    print(current_family)\n",
    "    X_current_family = X_train_last_fold[X_train_last_fold['family'] == current_family].drop(columns=['date', 'sales', 'family'])\n",
    "    y_current_family = y_train_last_fold.loc[X_current_family.index]\n",
    "    \n",
    "    lgbm_model = lgb.LGBMRegressor()\n",
    "    lgbm_model.fit(X_current_family, np.log1p(y_current_family), categorical_feature=['store_nbr'])\n",
    "    explainer = shap.TreeExplainer(lgbm_model)\n",
    "    shap_values_current_family = explainer(X_current_family)\n",
    "    shap.summary_plot(shap_values_current_family)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab8920b",
   "metadata": {},
   "source": [
    "<b><p>3) Most of 'bad' categories aren't first need goods, whereas most of 'good' categories are first need goods. Because of this fact, one thing about Shap plots becomes clear: most of 'bad' categories have 'store_nbr' feature as the most important (by SHAP), whereas most of 'good' categories don't have such trend. It can be explained in this way: predictions of categories, which can't be named as first need, can be influenced by store (probably, somewhere such goods aren't sold at all), whereas first need goods are sold in most of stores. What can we do with this? Probably it could be helpful to try first need categories without 'store_nbr' feature.</p>\n",
    "   <p>4) Most of 'good' categories have a lot of samples where low 'onpromotion' values have very low shapley values. It seems logical: less promos -- less sales number. But most of 'bad' categories don't have such tendency. Low values of onpromotion feature mostly have negative shapley values, but not so low as in most of 'good' categories.</p></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa1e1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores_for_every_feature_value(X, y, feature, base_pipelines):\n",
    "    scores_for_every_feature_value = {}\n",
    "    zero_categories = ['baby care', 'books']\n",
    "    for current_feature_value in X[feature].unique():\n",
    "        X_current_feature_value = X[X[feature] == current_feature_value]\n",
    "        if feature != 'store_nbr':\n",
    "            X_current_feature_value = X_current_feature_value.drop(columns=[feature])\n",
    "        y_current_feature_value = y.loc[X_current_feature_value.index]\n",
    "        \n",
    "        modelling_pipeline = RecursiveTSEstimatorWithZeroCategories(base_pipelines=base_pipelines, split_key='family', \n",
    "                                                                    target_col='sales', zero_categories=zero_categories)\n",
    "    \n",
    "        scores = cross_validate(modelling_pipeline, X_current_feature_value, y_current_feature_value, cv=splitter, \n",
    "                                scoring=CV_METRICS, return_estimator=True, error_score='raise', n_jobs=-1)\n",
    "\n",
    "        scores_for_every_feature_value[current_feature_value] = format_cv_test_scores(scores, save_scores=True, \n",
    "                                                                                      print_scores=False)\n",
    "    return scores_for_every_feature_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3fc1d6",
   "metadata": {},
   "source": [
    "#### Compare scores for every store_nbr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8d0186",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_for_every_store_nbr = scores_for_every_feature_value(X, y, feature='store_nbr', base_pipelines=base_pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb3f45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores_for_every_feature_value(scores_for_every_store_nbr, 'store_nbr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3fcf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 200))\n",
    "for i, family in enumerate(X['family'].unique()):\n",
    "    plt.subplot(33, 3, 3 * i + 1)\n",
    "    average_sales = X[(X['family'] == family) & (X['store_nbr'] == 52)].groupby('date')['sales'].median()\n",
    "    trend = average_sales.rolling(window=365, center=True, min_periods=183).mean()\n",
    "    plt.ylabel('mean sales')\n",
    "    ax = average_sales.plot(alpha=0.5)\n",
    "    ax = trend.plot(ax=ax, linewidth=3)\n",
    "    plt.title(family, color='red')\n",
    "    \n",
    "    plt.subplot(33, 3, 3 * i + 2)\n",
    "    average_sales = X[(X['family'] == family) & (X['store_nbr'] == 18)].groupby('date')['sales'].median()\n",
    "    trend = average_sales.rolling(window=365, center=True, min_periods=183).mean()\n",
    "    plt.ylabel('mean sales')\n",
    "    ax = average_sales.plot(alpha=0.5)\n",
    "    ax = trend.plot(ax=ax, linewidth=3)\n",
    "    plt.title(family, color='red')\n",
    "    \n",
    "    plt.subplot(33, 3, 3 * i + 3)\n",
    "    average_sales = X[(X['family'] == family) & (X['store_nbr'] == 25)].groupby('date')['sales'].median()\n",
    "    trend = average_sales.rolling(window=365, center=True, min_periods=183).mean()\n",
    "    plt.ylabel('mean sales')\n",
    "    ax = average_sales.plot(alpha=0.5)\n",
    "    ax = trend.plot(ax=ax, linewidth=3)\n",
    "    plt.title(family, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f66a87",
   "metadata": {},
   "source": [
    "<b><p>5) Model makes mistakes in 52-nd store_nbr, because of the distribution of its sales (no data before ~2017 year, probably this store was opened very late); in 18-th, 25-th store_nbr because of no data during some period of time in the last year of observations (there is no such period in other categories).</p>\n",
    "    <p><i>Possible solution:</i> for 52-nd store_nbr train the model only on the recent non-zero data (since the store was opened), for 18-th, 25-th store_nbr fill 'no-data' samples using some rolling.</p></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479528c1",
   "metadata": {},
   "source": [
    "#### Compare scores for every store_city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1a2e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_copy = X.copy()\n",
    "X_copy = train_transformer.add_store_features(X_copy, columns_to_add=['city'])\n",
    "X_copy['store_city'] = LabelEncoder().fit_transform(X_copy['store_city']) + 1\n",
    "X_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3944fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_for_every_store_city = scores_for_every_feature_value(X_copy, y, feature='store_city', base_pipelines=base_pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ebe96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores_for_every_feature_value(scores_for_every_store_city, 'store_city')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5554fcc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 200))\n",
    "for i, family in enumerate(X_copy['family'].unique()):\n",
    "    plt.subplot(33, 3, 3 * i + 1)\n",
    "    average_sales = X_copy[(X_copy['family'] == family) & (X_copy['store_city'] == 21)].groupby('date')['sales'].median()\n",
    "    trend = average_sales.rolling(window=365, center=True, min_periods=183).mean()\n",
    "    plt.ylabel('mean sales')\n",
    "    ax = average_sales.plot(alpha=0.5)\n",
    "    ax = trend.plot(ax=ax, linewidth=3)\n",
    "    plt.title(f'{family}(21)', color='red')\n",
    "    \n",
    "    plt.subplot(33, 3, 3 * i + 2)\n",
    "    average_sales = X_copy[(X_copy['family'] == family) & (X_copy['store_city'] == 15)].groupby('date')['sales'].median()\n",
    "    trend = average_sales.rolling(window=365, center=True, min_periods=183).mean()\n",
    "    plt.ylabel('mean sales')\n",
    "    ax = average_sales.plot(alpha=0.5)\n",
    "    ax = trend.plot(ax=ax, linewidth=3)\n",
    "    plt.title(f'{family}(15)', color='red')\n",
    "    \n",
    "    plt.subplot(33, 3, 3 * i + 3)\n",
    "    average_sales = X_copy[(X_copy['family'] == family) & (X_copy['store_city'] == 7)].groupby('date')['sales'].median()\n",
    "    trend = average_sales.rolling(window=365, center=True, min_periods=183).mean()\n",
    "    plt.ylabel('mean sales')\n",
    "    ax = average_sales.plot(alpha=0.5)\n",
    "    ax = trend.plot(ax=ax, linewidth=3)\n",
    "    plt.title(f'{family}(7)', color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649769ef",
   "metadata": {},
   "source": [
    "<b><p>6) Big mistakes in 21-st and 15-th store_city can be explained by their distribution: 21-st store_city has some months with missing data (last year), 15-th store_city has an extremely fast sales raising in the last months of observations.</p>\n",
    "   <p>Possible solution: fill missing data with some rolling for 21-st store_city; use only last months for training for 15-th store_city.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6a8068",
   "metadata": {},
   "source": [
    "#### Compare scores for every store_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8931605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_copy = X.copy()\n",
    "X_copy = train_transformer.adding_stores_data(X_copy, columns_to_add=['cluster'])\n",
    "X_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fdda20",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_for_every_store_cluster = scores_for_every_feature_value(X_copy, y, feature='store_cluster', base_pipelines=base_pipelines)\n",
    "plot_scores_for_every_feature_value(scores_for_every_store_cluster, 'store_cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924a3b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_data = pd.read_csv(DATA_ROOT / 'stores.csv')\n",
    "stores_data['city'] = LabelEncoder().fit_transform(stores_data['city']) + 1\n",
    "stores_data['type'] = LabelEncoder().fit_transform(stores_data['type']) + 1\n",
    "# stores_data[(stores_data['cluster'] == 11) | (stores_data['cluster'] == 16) | (stores_data['cluster'] == 1)]\n",
    "\n",
    "stores_data[(stores_data['city'] == 21) | (stores_data['city'] == 15)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a91a53e",
   "metadata": {},
   "source": [
    "<b><p>7) Big mistakes in some clusters can be explained by the fact that every such cluster has 'bad' 'store_nbr' or 'bad' 'store_city' in their composition. To avoid such mistakes it is necessary to deal with 'store_nbr' and 'store_city' first.</p></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd030f3",
   "metadata": {},
   "source": [
    "## Top-N worst time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2a2fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_for_every_ts = {}\n",
    "tscv_every_ts = TimeSeriesSplit(gap=0, max_train_size=DAYS_IN_YEAR, n_splits=4, test_size=N_HORIZONS)\n",
    "\n",
    "for current_store_nbr in X['store_nbr'].unique():\n",
    "    for current_family in X['family'].unique():\n",
    "        print(current_store_nbr, current_family)\n",
    "        X_current_ts = X[(X['store_nbr'] == current_store_nbr) & (X['family'] == current_family)]\n",
    "        y_current_ts = y.loc[X_current_ts.index]\n",
    "        \n",
    "        base_pipeline = {current_family:base_pipelines[current_family]}\n",
    "        modelling_pipeline = RecursiveTSEstimatorWithZeroCategories(base_pipelines=base_pipeline, split_key='family', \n",
    "                                                                    target_col='sales', zero_categories=zero_categories)\n",
    "    \n",
    "        scores = cross_validate(modelling_pipeline, X_current_ts, y_current_ts, cv=tscv_every_ts, scoring=CV_METRICS, \n",
    "                                return_estimator=True, error_score='raise', n_jobs=-1)\n",
    "\n",
    "        scores_for_every_ts[f'{current_store_nbr}, {current_family}'] = format_cv_test_scores(scores, save_scores=True, \n",
    "                                                                                         print_scores=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d738b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(DATA_ROOT / 'every_ts_scores.txt') as f:\n",
    "    data = f.read()\n",
    "scores_for_every_ts = json.loads(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da55fa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsle_scores_every_ts = {k: v[0]['root_mean_squared_log_error'] for k, v in scores_for_every_ts.items()}\n",
    "rmsle_sorted_scores_every_ts = {k: v for k, v in sorted(rmsle_scores_every_ts.items(), \n",
    "                                                        key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d78d62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_rmsle_scores_ts = {k: v for k, v in rmsle_sorted_scores_every_ts.items() if v > 0.4185}\n",
    "bad_rmsle_scores_ts = {tuple(k.split(', ')): v for k, v in bad_rmsle_scores_ts.items()}\n",
    "print(len(bad_rmsle_scores_ts))\n",
    "bad_rmsle_scores_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da70b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time series with bad store_nbr (explained above)\n",
    "\n",
    "len([k for k in bad_rmsle_scores_ts.keys() if k[0] in ['52', '18', '25']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc69923",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_rmsle_scores_ts = {k: v for k, v in bad_rmsle_scores_ts.items() if k[0] not in ['52', '18', '25']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e589c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time series with two bad (and explained why) categories: 'school and office supplies', 'grocery ii'\n",
    "\n",
    "len([k for k in bad_rmsle_scores_ts.keys() if k[1] in ['school and office supplies', 'grocery ii']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc0c20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_rmsle_scores_ts = {k: v for k, v in bad_rmsle_scores_ts.items() if k[1] not in ['school and office supplies', 'grocery ii']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f94431e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero categories\n",
    "\n",
    "print('books:', len([k for k in bad_rmsle_scores_ts.keys() if k[1] == 'books']))\n",
    "print('baby care:', len([k for k in bad_rmsle_scores_ts.keys() if k[1] == 'baby care']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53210308",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_rmsle_scores_ts = {k: v for k, v in bad_rmsle_scores_ts.items() if k[1] not in ['books', 'baby care']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dd5929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time series with stores from 'bad' cities (also explained)\n",
    "\n",
    "stores_data = pd.read_csv(DATA_ROOT / 'stores.csv')\n",
    "stores_data['city'] = LabelEncoder().fit_transform(stores_data['city'])\n",
    "len([k for k in bad_rmsle_scores_ts.keys() if int(k[0]) in stores_data[stores_data['city'].isin([21, 15])]['store_nbr'].unique()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560fac8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_rmsle_scores_ts = {k: v for k, v in bad_rmsle_scores_ts.items() if int(k[0]) not in stores_data[stores_data['city'].isin([21, 15])]['store_nbr'].unique()}\n",
    "len(bad_rmsle_scores_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba673e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's leave only those time series, where rmsle > 0.66 (because probably these (bigger) mistakes \n",
    "# are the most important in those 569 which are left)\n",
    "# 0.66 is the bigger average mistake by category (lingerie)\n",
    "\n",
    "bad_rmsle_scores_ts = {k: v for k, v in bad_rmsle_scores_ts.items() if v > 0.66}\n",
    "len(bad_rmsle_scores_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9e493a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_categories = pd.Series([k[1] for k in bad_rmsle_scores_ts.keys()])\n",
    "bad_categories.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e8d72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_rmsle_scores_ts_sorted = {k: v for k, v in sorted(bad_rmsle_scores_ts.items(), \n",
    "                                                      key=lambda item: item[0][1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce78a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "for current_ts in bad_rmsle_scores_ts_sorted.keys():\n",
    "    print(f'{current_ts[0]}, {current_ts[1]}')\n",
    "    X_current_ts = X_train_last_fold[(X_train_last_fold['family'] == current_ts[1]) & \n",
    "                                     (X_train_last_fold['store_nbr'] == int(current_ts[0]))].drop(columns=['date', 'sales', 'family', 'store_nbr'])\n",
    "    y_current_ts = y_train_last_fold.loc[X_current_ts.index]\n",
    "    \n",
    "    lgbm_model = lgb.LGBMRegressor()\n",
    "    lgbm_model.fit(X_current_ts, np.log1p(y_current_ts))\n",
    "    explainer = shap.TreeExplainer(lgbm_model)\n",
    "    shap_values_current_ts = explainer(X_current_ts)\n",
    "    shap.summary_plot(shap_values_current_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f5abb0",
   "metadata": {},
   "source": [
    "<b><p>8) 'onpromotion' feature has a very small impact on model output (by SHAP) of the 'celebration' category in these summary_plots. It was shown above that LGBM feature_importance of 'onpromotion' for this category was also very small. Probably, this feature is the reason, why model makes mistakes on 'celebration' category and it is necessary to remove this feature for this category.</p>\n",
    "   <p><p>   The same situation with 'ladieswear', 'lawn and garden', 'lingerie' (in most stores) categories.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5a2ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_last_fold[X_train_last_fold['onpromotion'] > 0]['family'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6951ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_last_fold[(X_train_last_fold['onpromotion'] > 0) & (X_train_last_fold['family'] == 'hardware')].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad051ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc1688fe",
   "metadata": {},
   "source": [
    "### Waterfall plots for 'lingerie' category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6b9edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lingerie = X_train_last_fold[X_train_last_fold['family'] == 'lingerie'].drop(columns=['date', 'sales', 'family'])\n",
    "y_lingerie = y_train_last_fold.loc[X_lingerie.index]\n",
    "lgbm_model = lgb.LGBMRegressor()\n",
    "lgbm_model.fit(X_lingerie, np.log1p(y_lingerie), categorical_feature=['store_nbr'])\n",
    "explainer = shap.TreeExplainer(lgbm_model)\n",
    "shap_values_lingerie = explainer(X_lingerie)\n",
    "shap.summary_plot(shap_values_lingerie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb0e7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lingerie = X_train_last_fold[X_train_last_fold['family'] == 'lingerie'].reset_index()\n",
    "random_samples = X_lingerie.sample(n=50)\n",
    "for current_sample in random_samples.index:\n",
    "    print('date:', random_samples['date'][current_sample])\n",
    "    print('store_nbr:', random_samples['store_nbr'][current_sample])\n",
    "    print('index:', current_sample)\n",
    "    shap.plots.waterfall(shap_values_lingerie[current_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d965abc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5bdf97ec",
   "metadata": {},
   "source": [
    "### Changes in the best model according to the error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a24eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling missing values for 18-th and 25-th store_nbr using linear regression predictions\n",
    "\n",
    "X_nan_inds = X[X.isnull().any(axis=1)].index\n",
    "X = X.dropna().reset_index(drop=True)\n",
    "y = y.drop(index=X_nan_inds).reset_index(drop=True)\n",
    "\n",
    "missing_data_18 = X[(X['store_nbr'] == 18) & (X['date'] >= '2016-08-15') & (X['date'] <= '2016-12-02')].index\n",
    "missing_data_25 = X[(X['store_nbr'] == 25) & (X['date'] >= '2016-08-22') & (X['date'] <= '2016-10-26')].index\n",
    "\n",
    "\n",
    "base_pipelines_18_25 = {}\n",
    "for current_family in X['family'].unique():\n",
    "    base_pipelines_18_25[current_family] = LinearPipeline(cols_to_scale=['dcoilwtico'], drop_columns=['onpromotion'], \n",
    "                                                          lags=lags, split_key='family', target_col='sales', level='store_nbr')\n",
    "\n",
    "linear_pipeline = RecursiveTSEstimatorWithZeroCategories(split_key='family', target_col='sales', \n",
    "                                                         base_pipelines=base_pipelines_18_25, \n",
    "                                                         zero_categories=['baby care', 'books'])\n",
    "    \n",
    "X_train_linreg_18, X_train_linreg_25 = X[(X['store_nbr'] == 18) & (X['date'] >= '2015-08-15') & (X['date'] < '2016-08-15')], \\\n",
    "                                       X[(X['store_nbr'] == 25) & (X['date'] >= '2015-08-22') & (X['date'] < '2016-08-22')]\n",
    "y_train_linreg_18, y_train_linreg_25 = y.loc[X_train_linreg_18.index], y.loc[X_train_linreg_25.index]\n",
    "X_test_linreg_18, X_test_linreg_25 = X.loc[missing_data_18], X.loc[missing_data_25]\n",
    "\n",
    "linear_pipeline.fit(X_train_linreg_18, y_train_linreg_18)\n",
    "preds_18 = linear_pipeline.predict(X_test_linreg_18)\n",
    "X.loc[missing_data_18, 'sales'] = np.log1p(preds_18)\n",
    "y.loc[missing_data_18] = preds_18\n",
    "\n",
    "linear_pipeline.fit(X_train_linreg_25, y_train_linreg_25)\n",
    "preds_25 = linear_pipeline.predict(X_test_linreg_25)\n",
    "X.loc[missing_data_25, 'sales'] = np.log1p(preds_25)\n",
    "y.loc[missing_data_25] = preds_25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56549e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "for current_lag in lags:\n",
    "    X.loc[:, 'lag_{}'.format(current_lag)] = X.groupby(['store_nbr', 'family'])['sales'].shift(current_lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37930bf3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tscv_inner = TimeSeriesSplit(gap=0, max_train_size=(DAYS_IN_YEAR - 4 * N_HORIZONS) * N_STORES, n_splits=4,\n",
    "                             test_size=N_HORIZONS * N_STORES)\n",
    "train_indices = next(splitter.split(X, y))[0]\n",
    "X_train_first_fold, y_train_first_fold = X.iloc[train_indices], y.iloc[train_indices]\n",
    "\n",
    "best_cv_params_edited = tune_hyperparams(X_train_first_fold, y_train_first_fold, tscv_inner=tscv_inner, lags=lags, \n",
    "                                         level='store_nbr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409153c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_pipelines = {}\n",
    "for current_family in X['family'].unique():\n",
    "    if current_family in linear_categories:\n",
    "        base_pipelines[current_family] = LinearPipeline(cols_to_scale=['dcoilwtico'], cols_to_encode=['store_nbr'], \n",
    "                                                        drop_columns=['onpromotion'], lags=lags, split_key='family', \n",
    "                                                        target_col='sales', level='store_nbr')\n",
    "    else:\n",
    "        base_pipelines[current_family] = LGBMPipeline(lags=lags, split_key='family', target_col='sales', level='store_nbr',\n",
    "                                                      params=best_cv_params_edited[current_family], fit_params=fit_params)\n",
    "\n",
    "\n",
    "modelling_pipeline = RecursiveTSEstimatorWithZeroCategories(split_key='family', target_col='sales', \n",
    "                                                            base_pipelines=base_pipelines, \n",
    "                                                            zero_categories=['baby care', 'books'])\n",
    "\n",
    "scores = cross_validate(modelling_pipeline, X, y, cv=splitter, scoring=CV_METRICS, return_estimator=True, error_score='raise', \n",
    "                        n_jobs=-1)\n",
    "format_cv_test_scores(scores, metrics_to_plot=['root_mean_squared_log_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d520122b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 200))\n",
    "for i, family in enumerate(X['family'].unique()):\n",
    "    plt.subplot(33, 2, 2 * i + 1)\n",
    "    average_sales = X[(X['family'] == family) & (X['store_nbr'] == 18)].groupby('date')['sales'].median()\n",
    "    trend = average_sales.rolling(window=365, center=True, min_periods=183).mean()\n",
    "    plt.ylabel('mean sales')\n",
    "    ax = average_sales.plot(alpha=0.5)\n",
    "    ax = trend.plot(ax=ax, linewidth=3)\n",
    "    plt.title(family, color='red')\n",
    "    \n",
    "    plt.subplot(33, 2, 2 * i + 2)\n",
    "    average_sales = X[(X['family'] == family) & (X['store_nbr'] == 25)].groupby('date')['sales'].median()\n",
    "    trend = average_sales.rolling(window=365, center=True, min_periods=183).mean()\n",
    "    plt.ylabel('mean sales')\n",
    "    ax = average_sales.plot(alpha=0.5)\n",
    "    ax = trend.plot(ax=ax, linewidth=3)\n",
    "    plt.title(family, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29528ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the predictions of the best model\n",
    "\n",
    "linear_categories = ['baby care', 'books', 'lawn and garden', 'home appliances']\n",
    "fit_params = {'feature_name': ['store_nbr', 'onpromotion', 'dcoilwtico', 'lag_1', 'lag_2', 'lag_4', 'lag_6', 'lag_7', 'lag_14'],\n",
    "              'categorical_feature': ['store_nbr']}\n",
    "\n",
    "X_train, y_train = cut_history(X=X, date_column='date', keep_interval=pd.Timedelta(days=DAYS_IN_YEAR), y=y)\n",
    "\n",
    "base_pipelines = {}\n",
    "for current_family in X['family'].unique():\n",
    "    if current_family in linear_categories:\n",
    "        base_pipelines[current_family] = LinearPipeline(cols_to_scale=['dcoilwtico'], cols_to_encode=['store_nbr'], \n",
    "                                                         drop_columns=['onpromotion'], lags=lags, split_key='family', \n",
    "                                                         target_col='sales', level='store_nbr')\n",
    "    else:\n",
    "        base_pipelines[current_family] = LGBMPipeline(lags=lags, split_key='family', target_col='sales', level='store_nbr',\n",
    "                                                       fit_params=fit_params, params=best_final_params[current_family])\n",
    "\n",
    "final_modelling_pipeline = RecursiveTSEstimatorWithZeroCategories(split_key='family', target_col='sales', \n",
    "                                                                  base_pipelines=base_pipelines, \n",
    "                                                                  zero_categories=['baby care', 'books'])\n",
    "final_modelling_pipeline.fit(X_train, y_train)\n",
    "submission = pd.read_csv(DATA_ROOT / 'sample_submission.csv')\n",
    "submission['sales'] = final_modelling_pipeline.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead272f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_store_52 = X_train[(X_train['store_nbr'] == 52) & (X_train['date'] >= '2017-04-20')]\n",
    "y_train_store_52 = y_train.loc[X_train_store_52.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d86edfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters tuning for 52-nd store_nbr\n",
    "\n",
    "tscv_inner = TimeSeriesSplit(gap=0, n_splits=2, max_train_size=X_train_store_52['date'].nunique() - 2 * N_HORIZONS,\n",
    "                             test_size=N_HORIZONS)\n",
    "best_params_52 = tune_hyperparams(X_train_store_52, y_train_store_52, tscv_inner=tscv_inner, level='store_nbr', lags=lags, \n",
    "                                  drop_columns={family: ['store_nbr'] for family in X['family'].unique()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75927eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "        \n",
    "with open(DATA_ROOT / 'best_params_52.txt') as f:\n",
    "    data = f.read()\n",
    "best_params_52 = json.loads(data)\n",
    "\n",
    "with open(DATA_ROOT / 'best_cv_params_edited.txt') as f:\n",
    "    data = f.read()\n",
    "best_cv_params_edited = json.loads(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d069ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making predictions for 52-nd store_nbr\n",
    "\n",
    "base_pipelines_52 = {}\n",
    "for current_family in X['family'].unique():\n",
    "    if current_family in linear_categories:\n",
    "        base_pipelines_52[current_family] = LinearPipeline(cols_to_scale=['dcoilwtico'], drop_columns=['onpromotion'], \n",
    "                                                           lags=lags, split_key='family', target_col='sales', level='store_nbr')\n",
    "    else:\n",
    "        base_pipelines_52[current_family] = LGBMPipeline(lags=lags, split_key='family', target_col='sales', level='store_nbr',\n",
    "                                                         params=best_params_52[current_family])\n",
    "\n",
    "modelling_pipeline_52 = RecursiveTSEstimatorWithZeroCategories(split_key='family', target_col='sales', \n",
    "                                                               base_pipelines=base_pipelines_52, \n",
    "                                                               zero_categories=['baby care', 'books'])\n",
    "modelling_pipeline_52.fit(X_train_store_52, y_train_store_52)\n",
    "preds_store_52 = modelling_pipeline_52.predict(test_data[test_data['store_nbr'] == 52])\n",
    "submission.loc[test_data[test_data['store_nbr'] == 52].index, 'sales'] = preds_store_52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d150fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(DATA_ROOT / 'best_model_preds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6625f125",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db3f069",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_for_every_store_nbr = scores_for_every_feature_value(X, y, feature='store_nbr', base_pipelines=base_pipelines)\n",
    "plot_scores_for_every_feature_value(scores_for_every_store_nbr, 'store_nbr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3aa0160",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
